{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using wandb to track experiments.\n",
        "\n",
        "Demo task: multi-class image classification using CIFAR10 dataset."
      ],
      "metadata": {
        "id": "h6hjehhHW4tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import average_precision_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models\n",
        "from torchvision import transforms as T\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "OUnLA0ASMK4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgylenuzXoYH",
        "outputId": "b0926349-7d6c-4137-d257-b2e763265835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.24.0-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.5/206.5 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=f0eb49b8d654a412ea5bccee51148a6713f203b9124b0359020b2dd75ac40a14\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.24.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BG5lpYn4XvFx",
        "outputId": "a07eaa46-1593-4313-8975-157652123dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "lpr8-mXxXjKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The next cell includes-\n",
        "- Collecting the CIFAR10 dataset and defining data loaders.\n",
        "- Methods to load model, criterion, optimizer and schedulers.\n",
        "- Definition of AverageMeter"
      ],
      "metadata": {
        "id": "s8hEreACXETi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading CIFAR10 dataset\n",
        "inp_transforms = T.Compose([T.ToTensor(),\n",
        "                            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                        std=[0.229, 0.224, 0.225])])\n",
        "tgt_transforms = T.Lambda(lambda y: torch.zeros(10, dtype=torch.long).scatter_(0, torch.tensor(y), value=1))\n",
        "cifar10 = datasets.CIFAR10(root = \"/.\",\n",
        "                           transform = inp_transforms,\n",
        "                           target_transform = tgt_transforms,\n",
        "                           download = True)\n",
        "\n",
        "# Defining dataset split (80-20)\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(cifar10,\n",
        "                                                           [int(len(cifar10)*0.80), int(len(cifar10)*0.20)])\n",
        "\n",
        "# Defining the dataloaders\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              batch_size=200,\n",
        "                              shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset,\n",
        "                            batch_size=200,\n",
        "                            shuffle=False)\n",
        "\n",
        "\n",
        "# Method to get model based on config param model_type\n",
        "def get_model(model_type):\n",
        "    model = None\n",
        "    if model_type == \"pretrained\": # Loading pretrained ResNet18 and with updated to final fc layer. \n",
        "        model = models.resnet18(pretrained=True)\n",
        "        model.fc = nn.Linear(512, 10)\n",
        "        model = model.to(device)\n",
        "    elif model_type == \"scratch\": # Loading a blank ResNet18 which generated 10 outputs.\n",
        "        model = models.resnet18(num_classes=10)\n",
        "        model = model.to(device)\n",
        "    else:\n",
        "        raise NotImplemented\n",
        "    return model\n",
        "\n",
        "\n",
        "# Method to get criterion, optimizer and scheduler based on config params.\n",
        "def get_criterion_optimizer_scheduler(config, model):\n",
        "    optim_dct = {\n",
        "        \"adam\": optim.Adam,\n",
        "        \"SGD\": optim.SGD,\n",
        "        \"RMSprop\": optim.RMSprop\n",
        "    }\n",
        "    optimizer = optim_dct[config[\"optimizer\"]](model.parameters(), lr=config[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                           factor=0.1,\n",
        "                                                           patience=config[\"scheduler_patience\"],\n",
        "                                                           threshold=config[\"scheduler_thresh\"])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    return criterion, optimizer, scheduler\n",
        "\n",
        "\n",
        "\n",
        "# Remainder of this cell includes definition of AverageMeter (can be ignored)\n",
        "\"\"\"\n",
        "Code taken from Pytorch ImageNet examples\n",
        "https://github.com/pytorch/examples/blob/main/imagenet/main.py#L375\n",
        "\"\"\"\n",
        "class Summary():\n",
        "    NONE = 0\n",
        "    AVERAGE = 1\n",
        "    SUM = 2\n",
        "    COUNT = 3\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f', summary_type=Summary.AVERAGE):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.summary_type = summary_type\n",
        "        self.val_history = list()\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.val_history = list()\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        self.val_history.append(val)\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "    \n",
        "    def summary(self):\n",
        "        fmtstr = ''\n",
        "        if self.summary_type is Summary.NONE:\n",
        "            fmtstr = ''\n",
        "        elif self.summary_type is Summary.AVERAGE:\n",
        "            fmtstr = '{name} {avg:.3f}'\n",
        "        elif self.summary_type is Summary.SUM:\n",
        "            fmtstr = '{name} {sum:.3f}'\n",
        "        elif self.summary_type is Summary.COUNT:\n",
        "            fmtstr = '{name} {count:.3f}'\n",
        "        else:\n",
        "            raise ValueError('invalid summary type %r' % self.summary_type)        \n",
        "        return fmtstr.format(**self.__dict__)\n"
      ],
      "metadata": {
        "id": "N0IyFq2jMUTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4802b410-9b07-4b17-974b-117e7cfda52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 13084211.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /./cifar-10-python.tar.gz to /.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Following cell includes-\n",
        "- Defining the train and eval loops.\n",
        "- Method to trigger training loops based on config parameters."
      ],
      "metadata": {
        "id": "UIEngQlXXfi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The train function without wandb logging\n",
        "\n",
        "def train(model, criterion, optimizer, scheduler, epochs, train_dataloader, val_dataloader, device):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        loss_meter = AverageMeter(\"train_loss\", \":.5f\")\n",
        "        epoch_outs, epoch_tgt = list(), list()\n",
        "        for data, tgt_vec in tqdm(train_dataloader):\n",
        "            data, tgt_vec = data.to(device), tgt_vec.to(device)\n",
        "            targets = torch.argmax(tgt_vec, axis=1)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = criterion(out, targets)\n",
        "            loss_meter.update(loss.item(), data.shape[0])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_outs.append(out)\n",
        "            epoch_tgt.append(tgt_vec)\n",
        "        predictions = torch.vstack([torch.softmax(out, axis=1) for out in epoch_outs]).detach().cpu().numpy()\n",
        "        targets = torch.cat([tgt for tgt in epoch_tgt], dim=0).detach().cpu().numpy()\n",
        "        ap_score = average_precision_score(targets, predictions)\n",
        "        eval_loss_meter, eval_ap_score = evaluate(model, criterion, val_dataloader, device)\n",
        "        data_to_log = {\n",
        "            \"epoch\": epoch+1,\n",
        "            \"train_loss\": loss_meter.avg,\n",
        "            \"eval_loss\": eval_loss_meter.avg,\n",
        "            \"train_ap_score\": ap_score,\n",
        "            \"eval_ap_score\": eval_ap_score,\n",
        "            \"lr\": optimizer.state_dict()[\"param_groups\"][0][\"lr\"],\n",
        "        }\n",
        "        scheduler.step(eval_loss_meter.avg)\n",
        "        wandb.log(data_to_log)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, criterion, val_dataloader, device):\n",
        "    model.eval()\n",
        "    loss_meter = AverageMeter(\"eval_loss\", \":.5f\")\n",
        "    epoch_outs, epoch_tgt = list(), list()\n",
        "    for data, tgt_vec in val_dataloader:\n",
        "        data, tgt_vec = data.to(device), tgt_vec.to(device)\n",
        "        targets = torch.argmax(tgt_vec, axis=1)\n",
        "        out = model(data)\n",
        "        loss = criterion(out, targets)\n",
        "        loss_meter.update(loss.item(), data.shape[0])\n",
        "        epoch_outs.append(out)\n",
        "        epoch_tgt.append(tgt_vec)\n",
        "    predictions = torch.vstack([torch.softmax(out, axis=1) for out in epoch_outs]).detach().cpu().numpy()\n",
        "    targets = torch.cat([tgt for tgt in epoch_tgt], dim=0).detach().cpu().numpy()\n",
        "    ap_score = average_precision_score(targets, predictions)\n",
        "    return loss_meter, ap_score\n",
        "\n",
        "\n",
        "def trigger_training(config):\n",
        "    model = get_model(config[\"model_type\"])\n",
        "    criterion, optimizer, scheduler = get_criterion_optimizer_scheduler(config, model)\n",
        "    epochs = config[\"num_epochs\"]\n",
        "\n",
        "    train(model, criterion, optimizer, scheduler, epochs, train_dataloader, val_dataloader, device)\n"
      ],
      "metadata": {
        "id": "jfchRAmxMcoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete the config file, edit the cells in this notebook to log data to wandb and trigger training loops!"
      ],
      "metadata": {
        "id": "Jtb0ll98YTB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill the Config file below and log the experiment at wandb\n",
        "config = {\n",
        "    \"lr\": 0.001, \n",
        "    \"model_type\": \"pretrained\", # pretrained/scratch\n",
        "    \"optimizer\": \"adam\", # adam/SGD/RMSprop\n",
        "    \"criterion\": \"ce\",\n",
        "    \"scheduler_patience\": 3,\n",
        "    \"scheduler_thresh\": 0.001,\n",
        "    \"num_epochs\": 30, # CHANGE\n",
        "    \"gpu_id\": 0,\n",
        "    \"wandb_run_name\": \"haran-lr1e3\" ### FILL YOUR NAME HERE\n",
        "}\n"
      ],
      "metadata": {
        "id": "2XWOBnxhMkdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(entity = \"dhruv_sri\",   # wandb username. (NOT REQUIRED ARG. ANYMORE, it fetches from initial login)\n",
        "           project = \"wandb_demo\", # wandb project name. New project will be created if given project is missing.\n",
        "           config = config         # Config dict\n",
        "          )\n",
        "wandb.run.name = config[\"wandb_run_name\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "SR8LwWV7YGk2",
        "outputId": "acb0a1fd-fdc1-4c3a-93b6-8c5e74f1aec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230527_085353-6b14adgc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dhruv_sri/wandb_demo/runs/6b14adgc' target=\"_blank\">electric-wildflower-128</a></strong> to <a href='https://wandb.ai/dhruv_sri/wandb_demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dhruv_sri/wandb_demo' target=\"_blank\">https://wandb.ai/dhruv_sri/wandb_demo</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dhruv_sri/wandb_demo/runs/6b14adgc' target=\"_blank\">https://wandb.ai/dhruv_sri/wandb_demo/runs/6b14adgc</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigger_training(config)\n"
      ],
      "metadata": {
        "id": "BLGywkQ4NDMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a819cef2-c6bd-409c-d717-1b0d5f56b1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 200/200 [00:16<00:00, 11.88it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.21it/s]\n",
            "100%|██████████| 200/200 [00:17<00:00, 11.63it/s]\n",
            "100%|██████████| 200/200 [00:17<00:00, 11.74it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.38it/s]\n",
            "100%|██████████| 200/200 [00:15<00:00, 12.55it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.47it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.40it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.42it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.09it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.45it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 11.94it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.42it/s]\n",
            "100%|██████████| 200/200 [00:18<00:00, 10.82it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.43it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.08it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.33it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.12it/s]\n",
            "100%|██████████| 200/200 [00:15<00:00, 12.62it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.36it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.44it/s]\n",
            "100%|██████████| 200/200 [00:15<00:00, 12.57it/s]\n",
            "100%|██████████| 200/200 [00:17<00:00, 11.57it/s]\n",
            "100%|██████████| 200/200 [00:15<00:00, 12.52it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.28it/s]\n",
            "100%|██████████| 200/200 [00:15<00:00, 12.73it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 11.79it/s]\n",
            "100%|██████████| 200/200 [00:15<00:00, 12.63it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 11.95it/s]\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.37it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "RyGt96kZN60-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "231dc4eb-670e-45e8-b369-122b4a30edcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>eval_ap_score</td><td>▁▄▄▆▅▄▅▅██████████████████████</td></tr><tr><td>eval_loss</td><td>▄▂▂▁▃▄▄▅▃▄▆▇▇▇▇███████████████</td></tr><tr><td>lr</td><td>████████▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_ap_score</td><td>▁▅▆▇▇▇████████████████████████</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>eval_ap_score</td><td>0.90993</td></tr><tr><td>eval_loss</td><td>0.87554</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>train_ap_score</td><td>1.0</td></tr><tr><td>train_loss</td><td>0.00236</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">electric-wildflower-128</strong> at: <a href='https://wandb.ai/dhruv_sri/wandb_demo/runs/6b14adgc' target=\"_blank\">https://wandb.ai/dhruv_sri/wandb_demo/runs/6b14adgc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230527_085353-6b14adgc/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0RfnKkXnN7Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7XYU1R7SN7at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1u1MT1lHN7id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WandB Steps"
      ],
      "metadata": {
        "id": "4br2c7d1N8ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 1: Import WandB in your code\n",
        "\n",
        "import wandb\n",
        "\n",
        "### Step 1 ends"
      ],
      "metadata": {
        "id": "gg1VnOOBOAGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 2:\n",
        "# Initiate wandb in your script. The moment we trigger wandb.init(), an active\n",
        "# socket connection is established between your machine and wandb server.\n",
        "# We specify the entity (wandb username) and project (which wandb project to use for logging)\n",
        "\n",
        "wandb.init(entity = \"dhruv_sri\",   # wandb username. (NOT REQUIRED ARG. ANYMORE, it fetches from initial login)\n",
        "           project = \"wandb_demo\", # wandb project name. New project will be created if given project is missing.\n",
        "           config = config         # Config dict\n",
        "          )\n",
        "wandb.run.name = config[\"wandb_run_name\"]\n",
        "\n",
        "### Step 2 ends.\n"
      ],
      "metadata": {
        "id": "2oIAE7tpOBH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 3: Trigger wandb log\n",
        "# This step is responsible for sending the logs to wandb\n",
        "\n",
        "wandb.log(data_to_log)\n",
        "\n",
        "### Step 3 ends.\n"
      ],
      "metadata": {
        "id": "P_c8_juPOC6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 4 (Optional)\n",
        "# This closes the active socket connection to wandb server. Optional since wandb destructor does the same.\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "### Step 4 ends.\n"
      ],
      "metadata": {
        "id": "JnQzf5xHOEKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-fQ5XateZfx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GY131mhVZfvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WandB sweeps related steps"
      ],
      "metadata": {
        "id": "UiUK11qqZQZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 1:\n",
        "# Create a WandB sweep config file.\n",
        "# This config file will be used at the WandB website to initialize a sweep server\n",
        "program: \"demo.py\"\n",
        "method: \"grid\"\n",
        "metric:\n",
        "  name: \"eval_ap_score\"\n",
        "  goal: \"maximize\"\n",
        "parameters:\n",
        "    criterion:\n",
        "      value: \"ce\"\n",
        "    gpu_id:\n",
        "      value: 0\n",
        "    lr:\n",
        "      values: [0.1, 0.001, 0.0001]\n",
        "    model_type:\n",
        "      values: [\"scratch\", \"pretrained\"]\n",
        "    num_epochs:\n",
        "      value: 25\n",
        "    optimizer:\n",
        "      values: [\"adam\", \"SGD\", \"RMSprop\"]\n",
        "    scheduler_patience:\n",
        "      value: 3\n",
        "    scheduler_thresh:\n",
        "      value: 0.01\n",
        "\n",
        "        \n",
        "### A sample sweep config file if bayes method is used-\n",
        "# program: wandb_demo.py\n",
        "# method: bayes\n",
        "# metric:\n",
        "#   name: \"eval_ap_score\"\n",
        "#   goal: maximize\n",
        "# parameters:\n",
        "#   lr:\n",
        "#     distribution: uniform\n",
        "#     min: 0.00001\n",
        "#     max: 0.1\n",
        "#   criterion:\n",
        "#     distribution: categorical\n",
        "#     value:\n",
        "#       - ce\n",
        "#   optimizer:\n",
        "#     distribution: categorical\n",
        "#     values:\n",
        "#       - adam\n",
        "#       - SGD\n",
        "#       - RMSprop\n",
        "#   model_type:\n",
        "#     distribution: categorical\n",
        "#     values:\n",
        "#       - pretrained\n",
        "#       - scratch\n",
        "#   num_epochs:\n",
        "#     value:\n",
        "#       - 30\n",
        "#   scheduler_thresh:\n",
        "#     distribution: uniform\n",
        "#     min: 0.001\n",
        "#     max: 0.01\n",
        "#   scheduler_patience:\n",
        "#     distribution: int_uniform\n",
        "#     min: 2\n",
        "#     max: 10\n"
      ],
      "metadata": {
        "id": "tMbRxfDwZT8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 2\n",
        "# After using the above config on wandb website, you will get a sweep id in return.\n",
        "# E.g. sweep id- dhruv_sri/wandb_demo/hbyp0tl8\n",
        "#\n",
        "# Add the following agent line in your code-\n",
        "# Use the generated sweep id in the below code\n",
        "\n",
        "wandb.agent(sweep_id=\"### FILL SWEEP ID HERE ###\", function=sweep_agent_manager, count=100)\n"
      ],
      "metadata": {
        "id": "8yEkgDnGZT2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 3\n",
        "# Notice in above command we mentioned an argument named \"function\"\n",
        "# Wandb agents must trigger a function where they can initiate a socket to wandb and get a config.\n",
        "# So, we will use the following sweep_agent_manager function here-\n",
        "\n",
        "def sweep_agent_manager():\n",
        "    wandb.init()\n",
        "    config = dict(wandb.config)\n",
        "    run_name = f\"{config['model_type']}_{config['optimizer']}_{config['lr']}\"\n",
        "    wandb.run.name = run_name\n",
        "    trigger_training(config)\n"
      ],
      "metadata": {
        "id": "bqY9TPyJZTx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Done.\n",
        "# Now execute your training script on multiple machines.\n",
        "# Each run will request the config file from wandb and related experiments will be logged.\n",
        "# \n",
        "# NOTE!! wandb.log(data_to_log) must be present inside the code!! Else there is no meaning to sweep.\n"
      ],
      "metadata": {
        "id": "alHqJ4mQZTtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ------------------------------ Ends ------------------------------"
      ],
      "metadata": {
        "id": "aNIxPqn_OGTU"
      }
    }
  ]
}